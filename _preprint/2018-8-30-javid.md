---
layout: post
title: Zero-shot Transfer Learning for Semantic Parsing
---

<table style="width:100%">
  <tr>
 	<th>
	    <img src="/images/m2m-o2m-e2d-cal.svg"/>
	</th>
	<th>
	    <img src="/images/m2m-o2m-e2d-pub.svg"/>
	</th>
  </tr>
</table>
This paper is one of my memorabale works in LILY group particularly with Alexander Fabbri.
In 2017 in John Lafferty's non-parametric estimation course we were introduced to the award winning paper by Percy Liang's group. 
There was quite a lot of material in deep learning but you can't find really that much works about openning this black box. 
Inspired by *'Understanding Black-box Predictions via Influence Functions'* we started expanding our running works on semantic parsing. 

In this blog post we don't need to invoke any complicated ideas in semantic parsing but i'mg going to give enough background for you to follow me. 
In classical definition, semantic parsing is defined as generating logical forms of a sequence of human language. 
You can think of this as outputing sequences of lambda expressions, SQL queries, or programming languages. 
Our prediction is obtained through a large number of text-text pairs interpreted as squence-sequence pairs in a neural network or markov chain setting.
In this paper we try to figure out how to use information in different domains in order to boost the performance of the current domain. 
Let's say you have many examples in publication domain but a few in education. For example 1000 pairs in first domain and 100 in the second one.
If I assume publications is partly related to education, use of it as training data seems almost necessary. 
But how much and which examples, this is the motivation of this paper. 

There has been decades of works in transfer learning. 
What do I mean by that is transfer learning is well-defined in literature but it is mostly data-driven and highly dependant on the size of data.
So first we propose a structure for transfer learning with reasonable amount of parameters and then extend this with a non-parametric function for data augmentation. 

Let's define $\hat{k}=$ the most probable domain label for state $s_j$ and context $c_j$: 
\begin{equation*}
  \hat{k}= \arg\max_k ~W_{t_k}[s_j,c_j].
\end{equation*}
We assume there is a $W$ through our data capturing shared encoding between domains where $W_{t_k}$ associated to a single domain $k$. 
That is certainly true that some pairs of examples are in domain $k_i$ while partially related to domain $k_j$.
The data speaks by itself and we expect $W_{t_k}$, the corrosponding row for domain $k$, ends up with the most relevant space for it.

What we are going to assume is the prediction class of a pair should be close enough to the gold label: 
\begin{equation*}
  \label{eq:loss}
  \mathcal{L} = &-\log p(y_{1:j}) + \frac{1}{2}||t_k-\hat{t}_k||^2+\frac{1}{2}||t_k-\hat{t}_{k'}||^2
\end{equation*}
the first term is simple back-propagation step for a pair of encoder-decoder. 
The second term is associated with distance after end of encoder and the final term is for end of decoder.
Under one interpretation of this, we put a set of controls over parametrs through minimizing these errors. 

I am about to describe the non-parametric function has been used to augment domain $k_1$ with less examples by examples of domain $k_2$:

$$\hat{\theta}_{\epsilon,z_j} = \arg\min_{\theta}\sum_{z_i\neq z_j}\frac{1}{n}\mathcal{L}(z_i,\theta) + \epsilon \mathcal{L}(z_j,\theta,t_k)$$

Inspired by influence functions in non-parametric statistics, we are about to us a noise function to apply an adversarial attack on a single example.
We expect most relevant examples exacerbate the performance the most.  
We can think of adding noise to a single example, exactly analogous to removing $z_j$ and testify it's impact on model performance.
As you can see in the figure, the proposed methods in limited number of training data, outperformrs many-to-many, one-to-many, and encoder-to-decoder baselines. 
In the paper we also showed that the proposed methods surpasses the simple aggregation approach of domains. 

| method | \#parameters | reference |
|o2o | $\mathcal{O}(d^2+dV)$  | Johnson:2016 |
|o2m | $\mathcal{O}(d^2+kdV)$ |  Fan:17 | 
|m2m | $\mathcal{O}((k+1)d^2+kdV)$ | Daume:2009|
|e2d | $\mathcal{O}(d^2+dV)$ | Herzig:17 |
|z-shot k*d |$\mathcal{O}(d^2+dV)$ | Our Method|

For anybody who is interested in details I refer you to our original paper below.

```
@article{dadashkarimi2018zero,
  title={Zero-shot Transfer Learning for Semantic Parsing},
  author={Dadashkarimi, Javid and Fabbri, Alexander and Tatikonda, Sekhar and Radev, Dragomir R},
  journal={arXiv preprint arXiv:1808.09889},
  year={2018}
}
``` 
