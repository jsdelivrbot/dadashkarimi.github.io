---
layout: post
title: Semi-supervised learning using Gaussian fields and harmonic functions
---
Semi-supervised learning has been widely studied in whole bunch of different fields.
The problem is defined by a set of labeled examples $(x_i,y_i)$ where $y_i$ is the label for a feature vector $x_i$ and a set of unlabeled examples $(x_j,f_j)$ where $f_j$ is our estimation for example $x_j$ which has no lable.
 The former set is in size of $L$ and the latter one in $U$.
The work by (Zhu, Ghahramani, and Lafferty,2003) probably is the most cited paper in this area. 
They claim that like what happens in Physics our network of examples remains stable while it minimizes an energy function:

\begin{equation}
E(f) = \frac{1}{2}\sum_{i,j} w_{i,j} (f(i)-f(j))^2
\label{eq:energy}
\end{equation}

where $w_{i,j}$ is an element in weight matrix $W[w_{i,j}]$ which is purely defined in terms of weights between $x_i$ and $x_j$. $y_i=f(i)$ if $x_i$ is a member of our labled set.
You don't need to go that far to guess why the authors mentioned gaussian field in the title of the paper:

\begin{equation*}
w_{i,j} = \exp(-\sum_{d=1}^{m} \frac{(x_{id}-x_{jd})^2}{\sigma_d^2})
\end{equation*}


The function that minimizes Eq. 1 should be harmonic: $\Delta f =0$ on unlabled data. Here $\Delta=W-D$ is the combinatorial laplacian where $D=diag(d_i)$ which is equal to $\sum_j W_{i,j}$. 
A harmonic solution for this problem is:

\begin{equation*}
f(j) = \frac{1}{d_j} \sum_{i~j} w_{i,j} f(i), j=L+1,..,L+U
\label{eq:average}
\end{equation*}

Since some neighbours of node $x_j$ is labled and some not, this problems is assumed to have an iterative solution.
They proposed a closed form solution since the harmonic function expected to have a unique and converged solution:

\begin{equation*}
f_u = (D_{uu} - W_{uu})^{-1} W_{ul} f_l = (I-P_{uu})^{-1}P_{ul}f_l
\label{eq:closed}
\end{equation*}

in which $P=D^{-1}W$ in equation $f=Pf$.
You can interpret this as a solution for a random walk problem where you start from a labled node and stop by an unlabled one through a number of random jumps and probabilty measurs. 
The solution proposed by this paper is a non-parametric estimation for function $f$.
In case the energy function truely representing the problem setting, this harmonic functions is the optimum solution. 
In the next blog post I will discuss about a new objective function that is naturally capturing the data distribution as well as taking into account randomness in span of time (Rosenfield and Globerson, 2018).

<center>
<img src="/images/semi-example-1.svg" width="500px"/>
</center>
Fig.1 what is the label of $x$? Data distribution affects the model's performance. In case A, a simple 1-nn assigns the blue lable for the subject. In case B, both classes have the same chance and in the last one 1-nn assings the red one.  



I hope you pick this article up in less than 7 minutes!

```
@inproceedings{Zhu:2003:,
 author = {Zhu, Xiaojin and Ghahramani, Zoubin and Lafferty, John},
 title = {Semi-supervised Learning Using Gaussian Fields and Harmonic Functions},
 booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
 series = {ICML'03},
 year = {2003},
 isbn = {1-57735-189-4},
 location = {Washington, DC, USA},
 pages = {912--919},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=3041838.3041953},
 acmid = {3041953},
 publisher = {AAAI Press},
} 
``` 

