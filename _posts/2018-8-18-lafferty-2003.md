---
layout: post
title: Semi-supervised learning using Gaussian fields and harmonic functions
---

|![_config.yml](/images/semi-example-1.svg)|
|Fig.1 Data distribution effects the model's performance. |

Semi-supervised learning has been widely studied in whole bunch of different fields.
The problem is defined by a set of examples (<img alt="$x_i$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/9fc20fb1d3825674c6a279cb0d5ca636.svg?sanitize=true" align="middle" width="13.993485pt" height="14.10255pt"/>,<img alt="$y_i$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/2b442e3e088d1b744730822d18e7aa21.svg?sanitize=true" align="middle" width="12.662925pt" height="14.10255pt"/>) where <img alt="$y_i$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/2b442e3e088d1b744730822d18e7aa21.svg?sanitize=true" align="middle" width="12.662925pt" height="14.10255pt"/> is the labeld assigned for a feature vector <img alt="$x_i$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/9fc20fb1d3825674c6a279cb0d5ca636.svg?sanitize=true" align="middle" width="13.993485pt" height="14.10255pt"/> and another set of examples (<img alt="$x_j$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/4d8443b72a1de913b4a3995119296c90.svg?sanitize=true" align="middle" width="15.44169pt" height="14.10255pt"/>,<img alt="$f_j$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/ac9424c220341fa74016e5769014f456.svg?sanitize=true" align="middle" width="14.099745pt" height="22.74591pt"/>) where <img alt="$f_j$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/ac9424c220341fa74016e5769014f456.svg?sanitize=true" align="middle" width="14.099745pt" height="22.74591pt"/> is our estimation for example <img alt="$x_j$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/4d8443b72a1de913b4a3995119296c90.svg?sanitize=true" align="middle" width="15.44169pt" height="14.10255pt"/>. The former set is in size of <img alt="$L$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/ddcb483302ed36a59286424aa5e0be17.svg?sanitize=true" align="middle" width="11.14542pt" height="22.38192pt"/> and the latter one is <img alt="$U$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/6bac6ec50c01592407695ef84f457232.svg?sanitize=true" align="middle" width="12.96735pt" height="22.38192pt"/>.
The work by (Zhu, Ghahramani, and Lafferty,2003) probably is the most cited paper in this area. 
They claim that like what happens in Physics our network of examples remains stable while it minimizes an energy function:

<p align="center"><img alt="\begin{equation}&#10;E(f) = \frac{1}{2}\sum_{i,j} w_{i,j} (f(i)-f(j))^2&#10;\label{eq:energy}&#10;\end{equation}" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/0d544b823be5462c7d4a1370d75f4caa.svg?sanitize=true" align="middle" width="462.594pt" height="43.298805pt"/></p>

where <img alt="$w_{i,j}$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/9982a9d682d08696452d15a2576d80da.svg?sanitize=true" align="middle" width="26.32938pt" height="14.10255pt"/> is the element in weight matrix <img alt="$W[w_{i,j}]$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/9d9080f511f5be2fa1f312bcaf250ae6.svg?sanitize=true" align="middle" width="54.054495pt" height="24.56553pt"/> between examples <img alt="$x_i$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/9fc20fb1d3825674c6a279cb0d5ca636.svg?sanitize=true" align="middle" width="13.993485pt" height="14.10255pt"/> and <img alt="$x_j$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/4d8443b72a1de913b4a3995119296c90.svg?sanitize=true" align="middle" width="15.44169pt" height="14.10255pt"/>. <img alt="$y_i=f(i)$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/3c9d2a7d972ebe86e4f1a668e853098d.svg?sanitize=true" align="middle" width="63.53919pt" height="24.56553pt"/> if <img alt="$x_i$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/9fc20fb1d3825674c6a279cb0d5ca636.svg?sanitize=true" align="middle" width="13.993485pt" height="14.10255pt"/> is a member of our labled set.
The function that minimizes Eq. \ref{eq:energy} should be harmonic: <img alt="$\Delta f =0$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/fbbe335d556381bcca142ab8cc528963.svg?sanitize=true" align="middle" width="53.49333pt" height="22.74591pt"/> on unlabled data. Here <img alt="$\Delta=W-D$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/df7cf4f80d1e788009c366761e3ae4ff.svg?sanitize=true" align="middle" width="87.324765pt" height="22.38192pt"/> is the combinatorial laplacian where <img alt="$D=diag(di)$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/c760236a1961fe63a657ec89cb577246.svg?sanitize=true" align="middle" width="94.012875pt" height="24.56553pt"/>. A harmonic solution for this problem is:

<p align="center"><img alt="\begin{equation}&#10;f(j) = \frac{1}{d_j} \sum_{i~j} w_{i,j} f(i), j=L+1,..,L+U&#10;\label{eq:average}&#10;\end{equation}" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/9d436c3833fdb7891d447e6e54211ff2.svg?sanitize=true" align="middle" width="500.49285pt" height="43.298805pt"/></p>

Since some neighbours of node <img alt="$x_j$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/4d8443b72a1de913b4a3995119296c90.svg?sanitize=true" align="middle" width="15.44169pt" height="14.10255pt"/> is labled and some not, this problems is assumed to have an iterative solution.
They proposed a closed form solution since the harmonic function should have a unique convergance:

<p align="center"><img alt="\begin{equation}&#10;f_u = (D_{uu}) - W_{uu})^{-1} W_{ul} f_l = (I-P_{uu})^{-1}P_{ul}f_l&#10;\label{eq:closed}&#10;\end{equation}" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/5a99d5fc2be72370f595bb49037d9d0a.svg?sanitize=true" align="middle" width="521.6211pt" height="18.269295pt"/></p>

in which <img alt="$P=D^{-1}W$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/65f368d03fb24f93db88242d82490565.svg?sanitize=true" align="middle" width="84.07278pt" height="26.70657pt"/> in equation <img alt="$f=Pf$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/fb818a4c6f777ab3c4794c5f7125b2d2.svg?sanitize=true" align="middle" width="54.235335pt" height="22.74591pt"/>.
You can interpret this as a solution for a random walk problem where you start from a labled node and stop by an unlabled one through a number of random jumps and probabilty measurs. 


```
@inproceedings{Zhu:2003:,
 author = {Zhu, Xiaojin and Ghahramani, Zoubin and Lafferty, John},
 title = {Semi-supervised Learning Using Gaussian Fields and Harmonic Functions},
 booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
 series = {ICML'03},
 year = {2003},
 isbn = {1-57735-189-4},
 location = {Washington, DC, USA},
 pages = {912--919},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=3041838.3041953},
 acmid = {3041953},
 publisher = {AAAI Press},
} 
``` 

