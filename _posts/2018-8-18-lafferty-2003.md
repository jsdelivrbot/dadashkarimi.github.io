---
layout: post
title: Semi-supervised learning using Gaussian fields and harmonic functions
---
Semi-supervised learning has been widely studied in whole bunch of different fields.
The problem is defined by a set of labeled examples <img alt="$(x_i,y_i)$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/45f2dbf90251d9796e488d974777c8c8.svg?invert_in_darkmode" align="middle" width="48.361335pt" height="24.56553pt"/> where <img alt="$y_i$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/2b442e3e088d1b744730822d18e7aa21.svg?invert_in_darkmode" align="middle" width="12.662925pt" height="14.10255pt"/> is the label for a feature vector <img alt="$x_i$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/9fc20fb1d3825674c6a279cb0d5ca636.svg?invert_in_darkmode" align="middle" width="13.993485pt" height="14.10255pt"/> and a set of unlabeled examples <img alt="$(x_j,f_j)$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/b8dd2f065bf7ccad2dc908c997b223b1.svg?invert_in_darkmode" align="middle" width="51.257085pt" height="24.56553pt"/> where <img alt="$f_j$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/ac9424c220341fa74016e5769014f456.svg?invert_in_darkmode" align="middle" width="14.099745pt" height="22.74591pt"/> is our estimation for example <img alt="$x_j$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/4d8443b72a1de913b4a3995119296c90.svg?invert_in_darkmode" align="middle" width="15.44169pt" height="14.10255pt"/> which has no lable.
 The former set is in size of <img alt="$L$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/ddcb483302ed36a59286424aa5e0be17.svg?invert_in_darkmode" align="middle" width="11.14542pt" height="22.38192pt"/> and the latter one in <img alt="$U$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/6bac6ec50c01592407695ef84f457232.svg?invert_in_darkmode" align="middle" width="12.96735pt" height="22.38192pt"/>.
The work by (Zhu, Ghahramani, and Lafferty,2003) probably is the most cited paper in this area. 
They claim that like what happens in Physics our network of examples remains stable while it minimizes an energy function:



<p align="center"><img alt="\begin{equation}&#10;E(f) = \frac{1}{2}\sum_{i,j} w_{i,j} (f(i)-f(j))^2&#10;\label{eq:energy}&#10;\end{equation}" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/0d544b823be5462c7d4a1370d75f4caa.svg?invert_in_darkmode" align="middle" width="462.594pt" height="43.298805pt"/></p>

where <img alt="$w_{i,j}$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/9982a9d682d08696452d15a2576d80da.svg?invert_in_darkmode" align="middle" width="26.32938pt" height="14.10255pt"/> is an element in weight matrix <img alt="$W[w_{i,j}]$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/9d9080f511f5be2fa1f312bcaf250ae6.svg?invert_in_darkmode" align="middle" width="54.054495pt" height="24.56553pt"/> which is purely defined in terms of weights between <img alt="$x_i$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/9fc20fb1d3825674c6a279cb0d5ca636.svg?invert_in_darkmode" align="middle" width="13.993485pt" height="14.10255pt"/> and <img alt="$x_j$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/4d8443b72a1de913b4a3995119296c90.svg?invert_in_darkmode" align="middle" width="15.44169pt" height="14.10255pt"/>. <img alt="$y_i=f(i)$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/3c9d2a7d972ebe86e4f1a668e853098d.svg?invert_in_darkmode" align="middle" width="63.53919pt" height="24.56553pt"/> if <img alt="$x_i$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/9fc20fb1d3825674c6a279cb0d5ca636.svg?invert_in_darkmode" align="middle" width="13.993485pt" height="14.10255pt"/> is a member of our labled set.
You don't need to go that far to guess why the authors mentioned gaussian field in the title of the paper:

<p align="center"><img alt="\begin{equation*}&#10;w_{i,j} = \exp(-\sum_{d=1}^{m} \frac{(x_{id}-x_{jd})^2}{\sigma_d^2})&#10;\end{equation*}" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/43747190fe30ecdeec79c6632eca22eb.svg?invert_in_darkmode" align="middle" width="217.96005pt" height="45.25554pt"/></p>


The function that minimizes Eq. 1 should be harmonic: <img alt="$\Delta f =0$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/fbbe335d556381bcca142ab8cc528963.svg?invert_in_darkmode" align="middle" width="53.49333pt" height="22.74591pt"/> on unlabled data. Here <img alt="$\Delta=W-D$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/df7cf4f80d1e788009c366761e3ae4ff.svg?invert_in_darkmode" align="middle" width="87.324765pt" height="22.38192pt"/> is the combinatorial laplacian where <img alt="$D=diag(d_i)$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/710bce74622c0db53d5b05bfbe541227.svg?invert_in_darkmode" align="middle" width="93.843585pt" height="24.56553pt"/> which is equal to <img alt="$\sum_j W_{i,j}$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/ea8aceee46f814d72d347a282315fa2c.svg?invert_in_darkmode" align="middle" width="57.08967pt" height="24.65793pt"/>. 
A harmonic solution for this problem is:

<p align="center"><img alt="\begin{equation*}&#10;f(j) = \frac{1}{d_j} \sum_{i~j} w_{i,j} f(i), j=L+1,..,L+U&#10;\label{eq:average}&#10;\end{equation*}" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/5cf8007671ca2de191c74e065ff9f032.svg?invert_in_darkmode" align="middle" width="300.1581pt" height="43.298805pt"/></p>

Since some neighbours of node <img alt="$x_j$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/4d8443b72a1de913b4a3995119296c90.svg?invert_in_darkmode" align="middle" width="15.44169pt" height="14.10255pt"/> is labled and some not, this problems is assumed to have an iterative solution.
They proposed a closed form solution since the harmonic function expected to have a unique and converged solution:

<p align="center"><img alt="\begin{equation*}&#10;f_u = (D_{uu} - W_{uu})^{-1} W_{ul} f_l = (I-P_{uu})^{-1}P_{ul}f_l&#10;\label{eq:closed}&#10;\end{equation*}" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/d2d4377e08a882195423662b52e77efd.svg?invert_in_darkmode" align="middle" width="335.24535pt" height="18.269295pt"/></p>

in which <img alt="$P=D^{-1}W$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/65f368d03fb24f93db88242d82490565.svg?invert_in_darkmode" align="middle" width="84.07278pt" height="26.70657pt"/> in equation <img alt="$f=Pf$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/fb818a4c6f777ab3c4794c5f7125b2d2.svg?invert_in_darkmode" align="middle" width="54.235335pt" height="22.74591pt"/>.
You can interpret this as a solution for a random walk problem where you start from a labled node and stop by an unlabled one through a number of random jumps and probabilty measurs. 
The solution proposed by this paper is a non-parametric estimation for function <img alt="$f$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/190083ef7a1625fbc75f243cffb9c96d.svg?invert_in_darkmode" align="middle" width="9.780705pt" height="22.74591pt"/>.
In case the energy function truely representing the problem setting, this harmonic functions is the optimum solution. 
In the next blog post I will discuss about a new objective function that is naturally capturing the data distribution as well as taking into account randomness in span of time (Rosenfield and Globerson, 2018).

<center>
<img src="/images/semi-example-1.svg" width="500px"/>
</center>
Fig.1 what is the label of <img alt="$x$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align="middle" width="9.359955pt" height="14.10255pt"/>? Data distribution affects the model's performance. In case A, a simple 1-nn assigns the blue lable for the subject. In case B, both classes have the same chance and in the last one 1-nn assings the red one.  



I hope you pick this article up in less than 7 minutes!

```
@inproceedings{Zhu:2003:,
 author = {Zhu, Xiaojin and Ghahramani, Zoubin and Lafferty, John},
 title = {Semi-supervised Learning Using Gaussian Fields and Harmonic Functions},
 booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
 series = {ICML'03},
 year = {2003},
 isbn = {1-57735-189-4},
 location = {Washington, DC, USA},
 pages = {912--919},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=3041838.3041953},
 acmid = {3041953},
 publisher = {AAAI Press},
} 
``` 

