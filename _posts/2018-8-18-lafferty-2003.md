---
layout: post
title: Semi-supervised learning using Gaussian fields and harmonic functions
---

![_config.yml](/images/semi-example-1.svg)
Fig.1 Data distribution affects the model's performance. In case A, a simple 1-nn assigns the blue lable for the subject. In case B, both classes have the same chance and in the last case 1-nn assings the red one.  

Semi-supervised learning has been widely studied in whole bunch of different fields.
The problem is defined by a set of examples ($x_i$,$y_i$) where $y_i$ is the labeld assigned for a feature vector $x_i$ and another set of examples ($x_j$,$f_j$) where $f_j$ is our estimation for example $x_j$ which has no lable.
 The former set is in size of $L$ and the latter one in $U$.
The work by (Zhu, Ghahramani, and Lafferty,2003) probably is the most cited paper in this area. 
They claim that like what happens in Physics our network of examples remains stable while it minimizes an energy function:

\begin{equation*}
E(f) = \frac{1}{2}\sum_{i,j} w_{i,j} (f(i)-f(j))^2
\label{eq:energy}
\end{equation*}

where $w_{i,j}$ is the element in weight matrix $W[w_{i,j}]$ between examples $x_i$ and $x_j$. $y_i=f(i)$ if $x_i$ is a member of our labled set.
The function that minimizes Eq. 1 should be harmonic: $\Delta f =0$ on unlabled data. Here $\Delta=W-D$ is the combinatorial laplacian where $D=diag(d_i)$ which is equal to $\sum_j W_{i,j}$. 
A harmonic solution for this problem is:

\begin{equation*}
f(j) = \frac{1}{d_j} \sum_{i~j} w_{i,j} f(i), j=L+1,..,L+U
\label{eq:average}
\end{equation*}

Since some neighbours of node $x_j$ is labled and some not, this problems is assumed to have an iterative solution.
They proposed a closed form solution since the harmonic function expected to have a unique and converged solution:

\begin{equation*}
f_u = (D_{uu}) - W_{uu})^{-1} W_{ul} f_l = (I-P_{uu})^{-1}P_{ul}f_l
\label{eq:closed}
\end{equation*}

in which $P=D^{-1}W$ in equation $f=Pf$.
You can interpret this as a solution for a random walk problem where you start from a labled node and stop by an unlabled one through a number of random jumps and probabilty measurs. 

I hope you pick this up less than 7 minutes!

```
@inproceedings{Zhu:2003:,
 author = {Zhu, Xiaojin and Ghahramani, Zoubin and Lafferty, John},
 title = {Semi-supervised Learning Using Gaussian Fields and Harmonic Functions},
 booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
 series = {ICML'03},
 year = {2003},
 isbn = {1-57735-189-4},
 location = {Washington, DC, USA},
 pages = {912--919},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=3041838.3041953},
 acmid = {3041953},
 publisher = {AAAI Press},
} 
``` 

