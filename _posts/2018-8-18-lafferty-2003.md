---
layout: post
title: Semi-supervised learning using Gaussian fields and harmonic functions
---

![_config.yml](/images/semi-example-1.svg)
Fig.1 Data distribution affects the model's performance. In case A, a simple 1-nn assigns the blue lable for the subject. In case B, both classes have the same chance and in the last case 1-nn assings the red one.  

Semi-supervised learning has been widely studied in whole bunch of different fields.
The problem is defined by a set of examples (<img alt="$x_i$" style="position:relative; top:7%;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/9fc20fb1d3825674c6a279cb0d5ca636.svg?sanitize=true"/>,<img alt="$y_i$" style="position:relative; top:7%;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/2b442e3e088d1b744730822d18e7aa21.svg?sanitize=true"/>) where <img alt="$y_i$" style="position:relative; top:7%;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/2b442e3e088d1b744730822d18e7aa21.svg?sanitize=true"/> is the labeld assigned for a feature vector <img alt="$x_i$" style="position:relative; top:7%;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/9fc20fb1d3825674c6a279cb0d5ca636.svg?sanitize=true"/> and another set of examples (<img alt="$x_j$" style="position:relative; top:7%;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/4d8443b72a1de913b4a3995119296c90.svg?sanitize=true"/>,<img alt="$f_j$" style="position:relative; top:7%;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/ac9424c220341fa74016e5769014f456.svg?sanitize=true"/>) where <img alt="$f_j$" style="position:relative; top:7%;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/ac9424c220341fa74016e5769014f456.svg?sanitize=true"/> is our estimation for example <img alt="$x_j$" style="position:relative; top:7%;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/4d8443b72a1de913b4a3995119296c90.svg?sanitize=true"/> which has no lable.
 The former set is in size of <img alt="$L$" style="position:relative; top:7%;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/ddcb483302ed36a59286424aa5e0be17.svg?sanitize=true"/> and the latter one in <img alt="$U$" style="position:relative; top:7%;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/6bac6ec50c01592407695ef84f457232.svg?sanitize=true"/>.
The work by (Zhu, Ghahramani, and Lafferty,2003) probably is the most cited paper in this area. 
They claim that like what happens in Physics our network of examples remains stable while it minimizes an energy function:

<p align="center"><img alt="\begin{equation*}&#10;E(f) = \frac{1}{2}\sum_{i,j} w_{i,j} (f(i)-f(j))^2&#10;\label{eq:energy}&#10;\end{equation*}" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/134cb49b7934a21bcac46228a1f17701.svg?sanitize=true" align="middle" width="223.71195pt" height="43.298805pt"/></p>

where <img alt="$w_{i,j}$" style="position:relative; top:7%;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/9982a9d682d08696452d15a2576d80da.svg?sanitize=true"/> is the element in weight matrix <img alt="$W[w_{i,j}]$" style="position:relative; top:7%;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/9d9080f511f5be2fa1f312bcaf250ae6.svg?sanitize=true"/> between examples <img alt="$x_i$" style="position:relative; top:7%;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/9fc20fb1d3825674c6a279cb0d5ca636.svg?sanitize=true"/> and <img alt="$x_j$" style="position:relative; top:7%;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/4d8443b72a1de913b4a3995119296c90.svg?sanitize=true"/>. <img alt="$y_i=f(i)$" style="position:relative; top:7%;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/3c9d2a7d972ebe86e4f1a668e853098d.svg?sanitize=true"/> if <img alt="$x_i$" style="position:relative; top:7%;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/9fc20fb1d3825674c6a279cb0d5ca636.svg?sanitize=true"/> is a member of our labled set.
The function that minimizes Eq. 1 should be harmonic: <img alt="$\Delta f =0$" style="position:relative; top:7%;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/fbbe335d556381bcca142ab8cc528963.svg?sanitize=true"/> on unlabled data. Here <img alt="$\Delta=W-D$" style="position:relative; top:7%;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/df7cf4f80d1e788009c366761e3ae4ff.svg?sanitize=true"/> is the combinatorial laplacian where <img alt="$D=diag(d_i)$" style="position:relative; top:7%;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/710bce74622c0db53d5b05bfbe541227.svg?sanitize=true"/> which is equal to <img alt="$\sum_j W_{i,j}$" style="position:relative; top:7%;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/ea8aceee46f814d72d347a282315fa2c.svg?sanitize=true"/>. 
A harmonic solution for this problem is:

<p align="center"><img alt="\begin{equation*}&#10;f(j) = \frac{1}{d_j} \sum_{i~j} w_{i,j} f(i), j=L+1,..,L+U&#10;\label{eq:average}&#10;\end{equation*}" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/5cf8007671ca2de191c74e065ff9f032.svg?sanitize=true" align="middle" width="300.1581pt" height="43.298805pt"/></p>

Since some neighbours of node <img alt="$x_j$" style="position:relative; top:7%;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/4d8443b72a1de913b4a3995119296c90.svg?sanitize=true"/> is labled and some not, this problems is assumed to have an iterative solution.
They proposed a closed form solution since the harmonic function expected to have a unique and converged solution:

<p align="center"><img alt="\begin{equation*}&#10;f_u = (D_{uu}) - W_{uu})^{-1} W_{ul} f_l = (I-P_{uu})^{-1}P_{ul}f_l&#10;\label{eq:closed}&#10;\end{equation*}" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/de11c2e5f048a5188878f4100ec56bb2.svg?sanitize=true" align="middle" width="341.61435pt" height="18.269295pt"/></p>

in which <img alt="$P=D^{-1}W$" style="position:relative; top:7%;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/65f368d03fb24f93db88242d82490565.svg?sanitize=true"/> in equation <img alt="$f=Pf$" style="position:relative; top:7%;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/fb818a4c6f777ab3c4794c5f7125b2d2.svg?sanitize=true"/>.
You can interpret this as a solution for a random walk problem where you start from a labled node and stop by an unlabled one through a number of random jumps and probabilty measurs. 

I hope you pick this up less than 7 minutes!

```
@inproceedings{Zhu:2003:,
 author = {Zhu, Xiaojin and Ghahramani, Zoubin and Lafferty, John},
 title = {Semi-supervised Learning Using Gaussian Fields and Harmonic Functions},
 booktitle = {Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
 series = {ICML'03},
 year = {2003},
 isbn = {1-57735-189-4},
 location = {Washington, DC, USA},
 pages = {912--919},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=3041838.3041953},
 acmid = {3041953},
 publisher = {AAAI Press},
} 
``` 

