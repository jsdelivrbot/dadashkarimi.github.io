---
layout: post
title: A submodular Approach to Create Individualized Parcellations of the Human Brain
---


<img src="/images/individualized-parcelation.svg?sanitize=true"/>

This paper proposes a new approach to represent a human brain with graph. 
You don't need to know that much fundamental to understand this blog post. 
First start with "individualized parcellations of brain". 
I'm going to spend a little bit of time talking about each of these words where it fits into the big picture. 

In classic definition, parcellation is meant to pin some areas of human brain corrosponding to particular functionality.
From practical point of view this functionality should translate into higher node activation during a task performance.
A traditional view for parcellation is based around a unified group-level network construction.
There is good reasons why people use this simple approach; it is comparable and also scalable.
But, there is no guarantee to preserve individual differences since a single functional atlas may not perfectly fit for all subjects.
This paper presents a new individualized parcellation which is still comparable among subjects and also scalable.   

The algorithm first starts with group-level parcellation and tries to find a non-linear map into the individualized space. 
Then it should be able to find local examplars for every parcel which are truelly representing the network with a constant approximation. 
These examplars or voxels are analogous to the nodes in original parcellation. 

The basic structure is fairly easy to understand but I'm going to give more insight into this algorithm.
There are various ways finding local representative exapmles in literature. 
We can think exactly analogous to the classical clustering in a way that examplars are cluster centers. 
It seems to be fairly strightforward finding <img alt="$k$" style="position:relative; top:2px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/63bb9849783d01d91403bc9a5fea12a2.svg?sanitize=true"/> examplars representing a network with <img alt="$V$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/a9a3a4a202d80326bda413b5562d5cd1.svg?sanitize=true"/> nodes. 
This is equivalent to minimize the following function:

<p align="center"><img alt="\begin{equation*}&#10;L(S) = \frac{1}{|V|} \sum_{v\in V} \operatorname*{min}_{e\in S} d(v,e)&#10;\end{equation*}" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/57507f369cdd88de7b24c9ae60125760.svg?sanitize=true" align="middle" width="189.8226pt" height="42.18621pt"/></p> 

in which <img alt="$d: V\times V \rightarrow R$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/75af9f1e53f51bb7d43e0a269eb72640.svg?sanitize=true"/> is the pairwise distance function. 
So <img alt="$L$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/ddcb483302ed36a59286424aa5e0be17.svg?sanitize=true"/> is purely defining the amount of information lost by examplars in <img alt="$S$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/e257acd1ccbe7fcb654708f1a866bfe9.svg?sanitize=true"/>.
You don't need to go that far to solve this NP-hard problem. 
Simply solve the following monotone submodular function:

<p align="center"><img alt="\begin{equation*}&#10;f(S) = L(v_0) - L(S \cup v_0)&#10;\end{equation*}" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/4ce90c05cbb8a649a8e2d8bb5b634342.svg?sanitize=true" align="middle" width="183.03285pt" height="16.376943pt"/></p>

I am about to describe this greedy algorithm which is able to reach out <img alt="$1-\frac{1}{e} \approx 0.63$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/b46de78229bbea80956828bd7672c96a.svg?sanitize=true"/> approximation. 
First I'm going to give a background on submodular functions to make you able to follow the rest of the post:
A function <img alt="$f: 2^V \rightarrow R$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/c3b418c77a2185211b74ec96df8bbda8.svg?sanitize=true"/> is submodular if adding an element from <img alt="$e \in V \\ B$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/5797aa51d95f43e7f4040aadf71ca282.svg?sanitize=true"/> to set <img alt="$A\subset B$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/df2b50fa32a9fc56c2b82e00b1789871.svg?sanitize=true"/> gives you more gain than adding the same for <img alt="$B$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/61e84f854bc6258d4108d08d4c4a0852.svg?sanitize=true"/>.
This is well known with diminishing return in literature.  
This function is monotone if <img alt="$f(A)\leq f(B)$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/ea3e88ab4c7c6bbf00b73c1dda90ed5a.svg?sanitize=true"/>.
What a greedy algorithm does is to find single element <img alt="$e_i^*$" style="position:relative; top:2px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/09ea97675fb14fed3278a445a5497699.svg?sanitize=true"/>:
<p align="center"><img alt="\begin{equation*}&#10;e_i^* = \operatorname*{arg max}_{e\in V} f(S_{i-1} \cup {e}) -f(S_{i-1}) &#10;\end{equation*}" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/37a892eb10c2a5ce5d409c1dc9e98b6d.svg?sanitize=true" align="middle" width="248.8893pt" height="26.949285pt"/></p>
 
jointly with <img alt="$S_{i-1}$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/e1ba7f65dbf7a9476a3aeb62c774cfd2.svg?sanitize=true"/> efficiently rendering <img alt="$V$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/a9a3a4a202d80326bda413b5562d5cd1.svg?sanitize=true"/> as time is going on and <img alt="$S$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/e257acd1ccbe7fcb654708f1a866bfe9.svg?sanitize=true"/> getting larger and larger.
Let's define cardinality constraint <img alt="$K$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/d6328eaebbcd5c358f426dbea4bdbf70.svg?sanitize=true"/> which forces <img alt="$|S|=K$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/84f732208e4c3c597db6fcbe53c45b14.svg?sanitize=true"/> corrosponding your budget limit. 
The authors in this paper set <img alt="$K=1$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/fd875db432e0720b3d70a7f3b15319b0.svg?sanitize=true"/> which is analogous to many to one mapping in each parcel. 
According to the experimental results this individualized parcellation turns out to be more efficient in prediction flow intelligence of a number of subjects. 
This paper won the best paper award of MICCAI 2017 and Mehraveh Salehi received the young scientist award as well.

I hope you pick this article up in less than 7 minutes!

```
@inproceedings{Salehi:2017,
  title={A submodular approach to create individualized parcellations of the human brain},
  author={Salehi, Mehraveh and Karbasi, Amin and Scheinost, Dustin and Constable, R Todd},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={478--485},
  year={2017},
  organization={Springer}
}
``` 

