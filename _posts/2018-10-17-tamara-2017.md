---
layout: post
title: Automated Scalable Bayesian Inference via Hilbert Coresets
---
<img src="images/fig-1.png "/>

Tamara Broderick, assistant professor at MIT gave a talk in Oct 17 2018 at Yale. I'm going to spend a little bit of time on making a summary on her talk. 
Let's take a look at the following equation:

<p align="center"><img alt="$$&#10;\pi(\theta) = \frac{1}{Z} \exp(\mathcal{L}(\theta))\pi_0(\theta)&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/91492ab74501760ebda77e5f23254365.svg?invert_in_darkmode" align="middle" width="179.81865pt" height="32.9901pt" style="position:relative;top:10px"/></p>
where:

<p align="center"><img alt="$$&#10;\mathcal{L}_n(\theta) = \log p(y_n| \theta) , \mathcal{L}(\theta) = \sum_{n=1}^{N} \mathcal{L}_n(\theta) , Z = \int \exp(\mathcal{L}(\theta)) \pi_0(\theta) \text{d}\theta , (1)&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/3f0b8e49bf693641e0e805ce90c43fa0.svg?invert_in_darkmode" align="middle" width="482.5293pt" height="47.60745pt" style="position:relative;top:10px"/></p>

In Bayesian theory, <img alt="$\pi(\theta)$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/6fa7cd273b9dae03a6c405b96d9c5cbe.svg?invert_in_darkmode" align="middle" width="30.919185pt" height="24.6576pt" style="position:relative;top:10px"/> is known as posterior probability, <img alt="$\pi_0(\theta)$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/8aa9108c2200ae4076470ce9adb904d0.svg?invert_in_darkmode" align="middle" width="37.70382pt" height="24.6576pt" style="position:relative;top:10px"/> is our prior on parameters, and <img alt="$\exp(\mathcal{L}(\theta))$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/b1ea63db5d1b3e764e3cfe8b75e09a10.svg?invert_in_darkmode" align="middle" width="70.19661pt" height="24.6576pt" style="position:relative;top:10px"/> is our likelihood.  
Recently John Lafferty in Data Mining and Machine Learning class mentioned that alsmot 25% of the machine learning papers are about bayesian inference.
It's really cool and really simple! Our model missing true parameters and we need to impute them after observing data. 
But computing <img alt="$Z$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/5b51bd2e6f329245d425b8002d7cf942.svg?invert_in_darkmode" align="middle" width="12.351075pt" height="22.38192pt" style="position:relative;top:10px"/> is really expensive and that is the main intuition behind Bayesian coreset which is a small weighted subset of the original dataset which is able to approximate it in a more efficient way.
So our aim here is to find a set of non-negative weights <img alt="$w = (w_n)_{n=1}^N$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/e4cc989c959ee0093d4efba23ab9afc4.svg?invert_in_darkmode" align="middle" width="92.40033pt" height="27.65697pt" style="position:relative;top:10px"/> such that:

<p align="center"><img alt="$$&#10;\mathcal{L}(w,\theta) = \sum_{n=1}^N w_n \mathcal{L}_n (\theta) \\&#10;s.t., |\mathcal{L}(w,\theta)- \mathcal{L}(\theta)\leq| \epsilon |\mathcal{L}(\theta)|, \forall \theta \in \Theta&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/99f551308ebf237eebb0a525e5192d26.svg?invert_in_darkmode" align="middle" width="434.12325pt" height="47.60745pt" style="position:relative;top:10px"/></p>

Huggins et alproposed weights for samples as follow:

<p align="center"><img alt="$$&#10;\sigma_n = \text{sup} |\frac{\mathcal{L}_n(\theta)}{\mathcal{L}(\theta)}|&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/b90e505c55a193dba69775f325c397d6.svg?invert_in_darkmode" align="middle" width="119.330145pt" height="38.834895pt" style="position:relative;top:10px"/></p>

and then suggested to take <img alt="$M$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/fb97d38bcc19230b0acd442e17db879c.svg?invert_in_darkmode" align="middle" width="17.73981pt" height="22.46574pt" style="position:relative;top:10px"/> independant draws with probability proportional to <img alt="$\sigma_n$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/28cf960b1f96e750df70968130f6b0db.svg?invert_in_darkmode" align="middle" width="17.519205pt" height="14.15535pt" style="position:relative;top:10px"/>:

<p align="center"><img alt="$$&#10;\sigma = \sum_{n=1}^{N} \sigma_n, (M_1,..,M_N) \sim \text{Mult}(M, (\frac{\sigma_n}{\sigma})_{n=1}^{N})), W_n = \frac{\sigma}{\sigma_n} \frac{M_n}{M}&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/2b0964934da890f84af138bedb519cdb.svg?invert_in_darkmode" align="middle" width="437.2269pt" height="47.60745pt" style="position:relative;top:10px"/></p>

There is not really that much fundamental you need to know to figure it out that <img alt="$\mathscr{E}[W_n]=1$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/fcdab563c7e2c08db88ec87357bdf89e.svg?invert_in_darkmode" align="middle" width="76.82466pt" height="24.6576pt" style="position:relative;top:10px"/> and <img alt="$\mathscr{E}[\mathcal{L}(w,\theta) ] = \mathcal{L}(\theta) $" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/55f80d456ef09ccdfbafd873084115d6.svg?invert_in_darkmode" align="middle" width="128.242785pt" height="24.6576pt" style="position:relative;top:10px"/>.
There is a proof that the coreset log-likelihood <img alt="$\mathcal{L}(w,\theta) $" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/9acd6d22f0da8977311fbbe36e7de4a7.svg?invert_in_darkmode" align="middle" width="51.81363pt" height="24.6576pt" style="position:relative;top:10px"/> satisfies Eq. 2 with high probability and <img alt="$\epsilon^2 = \mathcal{O}(\frac{1}{M})$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/1100b6e726fa96fc9584547b63daea13.svg?invert_in_darkmode" align="middle" width="80.008005pt" height="27.77577pt" style="position:relative;top:10px"/>.
The other interesting formulation for this is to sparse vector sum approximation:

<p align="center"><img alt="$$&#10;\underset{w \in \mathscr{R}^N}{\mathrm{argmin}} ||\mathcal{L}(w) - \mathcal{L} ||^2&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/59248ffbbd0cbc63a8413863084a0082.svg?invert_in_darkmode" align="middle" width="142.854855pt" height="29.913675pt" style="position:relative;top:10px"/></p>
<p align="center"><img alt="$$&#10;s.t. w\geq 0, \sum{n=1}^{N} \mathscr{1} [w_n &gt; 0] \leq M&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/42adb69ecfbec0c307269118f5b34796.svg?invert_in_darkmode" align="middle" width="259.24635pt" height="26.30166pt" style="position:relative;top:10px"/></p>
this theorem depends on quantitiy <img alt="$0 \leq \hat{\etha}\leq 2$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/54e235fec0678e136145da6f09c2eee1.svg?invert_in_darkmode" align="middle" width="60.273675pt" height="22.83138pt" style="position:relative;top:10px"/>:

<p align="center"><img alt="$$&#10;\hat{\eta} = \underset{n,m \in [N]}{\mathrm{max}} ||\frac{\mathcal{L}_n}{\sigma_n}-\frac{\mathcal{L}_m}{\sigma_m}||&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/648108588ff502a28395cd39c10ae317.svg?invert_in_darkmode" align="middle" width="172.6692pt" height="36.600135pt" style="position:relative;top:10px"/></p>
that indicates how well the vectors are aligned. If it's zero it means the error is zero since we can approximate all <img alt="$\mathcal{L}$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/47291815667dfe5994c54805102e144b.svg?invert_in_darkmode" align="middle" width="11.337975pt" height="22.46574pt" style="position:relative;top:10px"/> with single vector  <img alt="$\mathcal{L}_n$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/1818eca8fbb5a307ec0f5436121fd2b2.svg?invert_in_darkmode" align="middle" width="19.46406pt" height="22.46574pt" style="position:relative;top:10px"/>. 
So the uniform coreset construction with probability <img alt="$\geq 1-\delta$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/439d02a63594f85ba6b9263ef6057948.svg?invert_in_darkmode" align="middle" width="53.59002pt" height="22.83138pt" style="position:relative;top:10px"/> satisfies:

<p align="center"><img alt="$$&#10;||\mathcal{L}(W)-\mathcal{L}|| \leq \frac{\sigma}{M}(\text{dim}(\mathcal{L}_n)_{n=1}^N+\hat{\eta}\sqrt{2\log \frac{1}{\delta}})&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/51de9d63236bc70122f825efcff70b0a.svg?invert_in_darkmode" align="middle" width="326.45085pt" height="39.45249pt" style="position:relative;top:10px"/></p>

where <img alt="$\text{dim}(u_n)_{n=1}^N$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/68592ae4d05130e4fc75690abb2a82e7.svg?invert_in_darkmode" align="middle" width="83.310975pt" height="27.65697pt" style="position:relative;top:10px"/> of <img alt="$N$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/f9c4988898e7f532b9f826a75014ed3c.svg?invert_in_darkmode" align="middle" width="14.94405pt" height="22.38192pt" style="position:relative;top:10px"/> vectors in a normed vector space is the minimum value of <img alt="$d\in N$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/d4f3107d7bd94831a02b4d84d883972d.svg?invert_in_darkmode" align="middle" width="43.64712pt" height="22.83138pt" style="position:relative;top:10px"/> such that all vectors can be approximated using linear combinations of a set of <img alt="$d$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/2103f85b8b1477f430fc407cad462224.svg?invert_in_darkmode" align="middle" width="8.556075pt" height="22.83138pt" style="position:relative;top:10px"/> unit vectors <img alt="$(v_j)_{j=1}^N$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/aeb96d324f25e322cf5ff8421d01f4fb.svg?invert_in_darkmode" align="middle" width="50.428455pt" height="27.65697pt" style="position:relative;top:10px"/>, <img alt="$|v_j|=1$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/ac1625745434a3a019e26e2c8e6b5a34.svg?invert_in_darkmode" align="middle" width="54.16389pt" height="24.6576pt" style="position:relative;top:10px"/>.

```
@article{Campbell:2017,
  title={Automated Scalable Bayesian Inference via Hilbert Coresets},
  author={Campbell, Trevor and Broderick, Tamara},
  journal={arXiv preprint arXiv:1710.05053},
  year={2017}
}
```
