---
layout: post
title: Automated Scalable Bayesian Inference via Hilbert Coresets
---

Tamara Broderick, assistant professor at MIT gave a talk in Oct 17 2018 at Yale. I'm going to spend a little bit of time on making a summary on her talk. 
Let's take a look at the following equation:

<p align="center"><img alt="$$&#10;\pi(\theta) = \frac{1}{Z} \exp(\mathcal{L}(\theta))\pi_0(\theta)&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/91492ab74501760ebda77e5f23254365.svg?invert_in_darkmode" align="middle" width="179.81865pt" height="32.9901pt" style="position:relative;top:10px"/></p>
where:

<p align="center"><img alt="$$&#10;\mathcal{L}_n(\theta) = \log p(y_n| \theta) , \mathcal{L}(\theta) = \sum_{n=1}^{N} \mathcal{L}_n(\theta) , Z = \int \exp(\mathcal{L}(\theta)) \pi_0(\theta) \text{d}\theta&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/03c94fdb68892d1eb5ba1fbadcf0608d.svg?invert_in_darkmode" align="middle" width="454.2186pt" height="47.60745pt" style="position:relative;top:10px"/></p>

In Bayesian theory, <img alt="$\pi(\theta)$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/6fa7cd273b9dae03a6c405b96d9c5cbe.svg?invert_in_darkmode" align="middle" width="30.919185pt" height="24.6576pt" style="position:relative;top:10px"/> is known as posterior probability, <img alt="$\pi_0(\theta)$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/8aa9108c2200ae4076470ce9adb904d0.svg?invert_in_darkmode" align="middle" width="37.70382pt" height="24.6576pt" style="position:relative;top:10px"/> is our prior on parameters, and <img alt="$\exp(\mathcal{L}(\theta))$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/b1ea63db5d1b3e764e3cfe8b75e09a10.svg?invert_in_darkmode" align="middle" width="70.19661pt" height="24.6576pt" style="position:relative;top:10px"/> is our likelihood.  
Recently John Lafferty in Data Mining and Machine Learning class mentioned that alsmot 25% of the machine learning papers are about bayesian inference.
It's really cool and really simple! Our model missing true parameters and we need to impute them after observing data. 
But computing <img alt="$Z$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/5b51bd2e6f329245d425b8002d7cf942.svg?invert_in_darkmode" align="middle" width="12.351075pt" height="22.38192pt" style="position:relative;top:10px"/> is really expensive and that is the main intuition behind Bayesian coreset which is a small weighted subset of the original dataset which is able to approximate it in a more efficient way.
So our aim here is to find a set of non-negative weights <img alt="$w = (w_n)_{n=1}^N$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/e4cc989c959ee0093d4efba23ab9afc4.svg?invert_in_darkmode" align="middle" width="92.40033pt" height="27.65697pt" style="position:relative;top:10px"/> such that:

<p align="center"><img alt="$$&#10;\mathcal{L}(w,\theta) = \sum_{n=1}^N w_n \mathcal{L}_n (\theta) \\&#10;s.t., |\mathcal{L}(w,\theta)- \mathcal{L}(\theta)\leq| \epsilon |\mathcal{L}(\theta)|, \forall \theta \in \Theta&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/99f551308ebf237eebb0a525e5192d26.svg?invert_in_darkmode" align="middle" width="434.12325pt" height="47.60745pt" style="position:relative;top:10px"/></p>

Huggins et alproposed weights for samples as follow:

<p align="center"><img alt="$$&#10;\sigma_n = \text{sup} |\frac{\mathcal{L}_n(\theta)}{\mathcal{L}(\theta)}|&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/b90e505c55a193dba69775f325c397d6.svg?invert_in_darkmode" align="middle" width="119.330145pt" height="38.834895pt" style="position:relative;top:10px"/></p>

and then suggested to take <img alt="$M$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/fb97d38bcc19230b0acd442e17db879c.svg?invert_in_darkmode" align="middle" width="17.73981pt" height="22.46574pt" style="position:relative;top:10px"/> independant draws with probability proportional to <img alt="$\sigma_n$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/28cf960b1f96e750df70968130f6b0db.svg?invert_in_darkmode" align="middle" width="17.519205pt" height="14.15535pt" style="position:relative;top:10px"/>:

<p align="center"><img alt="$$&#10;\sigma = \sum_{n=1}^{N} \sigma_n, (M_1,..,M_N) \sim \text{Mult}(M, (\frac{\sigma_n}{\sigma})_{n=1}^{N})), W_n = \frac{\sigma}{\sigma_n} \frac{M_n}{M}&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/2b0964934da890f84af138bedb519cdb.svg?invert_in_darkmode" align="middle" width="437.2269pt" height="47.60745pt" style="position:relative;top:10px"/></p>

There is not really that much fundamental you need to know to figure it out that <img alt="$\mathcal{E}[W_n]=1$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/1ed896c4df68572e63f9442f7c2ea1ca.svg?invert_in_darkmode" align="middle" width="73.888485pt" height="24.6576pt" style="position:relative;top:10px"/> and then 

```
@article{Campbell:2017,
  title={Automated Scalable Bayesian Inference via Hilbert Coresets},
  author={Campbell, Trevor and Broderick, Tamara},
  journal={arXiv preprint arXiv:1710.05053},
  year={2017}
}
```
