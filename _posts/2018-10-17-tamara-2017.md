---
layout: post
title: Automated Scalable Bayesian Inference via Hilbert Coresets
---
<img src="images/fig-1.png"/>

Tamara Broderick, assistant professor at MIT gave a talk in Oct 17 2018 at Yale. I'm going to spend a little bit of time on making a summary on her talk. 
Let's take a look at the following equation:

<p align="center"><img alt="$$&#10;\pi(\theta) = \frac{1}{Z} \exp(\mathcal{L}(\theta))\pi_0(\theta)&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/91492ab74501760ebda77e5f23254365.svg?invert_in_darkmode" align="middle" width="179.81865pt" height="32.9901pt" style="position:relative;top:10px"/></p>
where:

<p align="center"><img alt="$$&#10;\mathcal{L}_n(\theta) = \log p(y_n| \theta) , \mathcal{L}(\theta) = \sum_{n=1}^{N} \mathcal{L}_n(\theta) , Z = \int \exp(\mathcal{L}(\theta)) \pi_0(\theta) \text{d}\theta , (1)&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/3f0b8e49bf693641e0e805ce90c43fa0.svg?invert_in_darkmode" align="middle" width="482.5293pt" height="47.60745pt" style="position:relative;top:10px"/></p>

In Bayesian theory, <img alt="$\pi(\theta)$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/6fa7cd273b9dae03a6c405b96d9c5cbe.svg?invert_in_darkmode" align="middle" width="30.919185pt" height="24.6576pt" style="position:relative;top:10px"/> is known as posterior probability, <img alt="$\pi_0(\theta)$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/8aa9108c2200ae4076470ce9adb904d0.svg?invert_in_darkmode" align="middle" width="37.70382pt" height="24.6576pt" style="position:relative;top:10px"/> is our prior on parameters, and <img alt="$\exp(\mathcal{L}(\theta))$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/b1ea63db5d1b3e764e3cfe8b75e09a10.svg?invert_in_darkmode" align="middle" width="70.19661pt" height="24.6576pt" style="position:relative;top:10px"/> is our likelihood.  
Recently John Lafferty in Data Mining and Machine Learning class mentioned that alsmot 25% of the machine learning papers are about bayesian inference.
It's really cool and really simple! Our model missing true parameters and we need to impute them after observing data. 
But computing <img alt="$Z$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/5b51bd2e6f329245d425b8002d7cf942.svg?invert_in_darkmode" align="middle" width="12.351075pt" height="22.38192pt" style="position:relative;top:10px"/> is really expensive and that is the main intuition behind Bayesian coreset which is a small weighted subset of the original dataset which is able to approximate it in a more efficient way.
So our aim here is to find a set of non-negative weights <img alt="$w = (w_n)_{n=1}^N$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/e4cc989c959ee0093d4efba23ab9afc4.svg?invert_in_darkmode" align="middle" width="92.40033pt" height="27.65697pt" style="position:relative;top:10px"/> such that:

<p align="center"><img alt="$$&#10;\mathcal{L}(w,\theta) = \sum_{n=1}^N w_n \mathcal{L}_n (\theta) \\&#10;s.t., |\mathcal{L}(w,\theta)- \mathcal{L}(\theta)\leq| \epsilon |\mathcal{L}(\theta)|, \forall \theta \in \Theta&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/99f551308ebf237eebb0a525e5192d26.svg?invert_in_darkmode" align="middle" width="434.12325pt" height="47.60745pt" style="position:relative;top:10px"/></p>

Huggins et alproposed weights for samples as follow:

<p align="center"><img alt="$$&#10;\sigma_n = \text{sup} |\frac{\mathcal{L}_n(\theta)}{\mathcal{L}(\theta)}|&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/b90e505c55a193dba69775f325c397d6.svg?invert_in_darkmode" align="middle" width="119.330145pt" height="38.834895pt" style="position:relative;top:10px"/></p>

and then suggested to take <img alt="$M$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/fb97d38bcc19230b0acd442e17db879c.svg?invert_in_darkmode" align="middle" width="17.73981pt" height="22.46574pt" style="position:relative;top:10px"/> independant draws with probability proportional to <img alt="$\sigma_n$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/28cf960b1f96e750df70968130f6b0db.svg?invert_in_darkmode" align="middle" width="17.519205pt" height="14.15535pt" style="position:relative;top:10px"/>:

<p align="center"><img alt="$$&#10;\sigma = \sum_{n=1}^{N} \sigma_n, (M_1,..,M_N) \sim \text{Mult}(M, (\frac{\sigma_n}{\sigma})_{n=1}^{N})), W_n = \frac{\sigma}{\sigma_n} \frac{M_n}{M}&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/2b0964934da890f84af138bedb519cdb.svg?invert_in_darkmode" align="middle" width="437.2269pt" height="47.60745pt" style="position:relative;top:10px"/></p>

There is not really that much fundamental you need to know to figure it out that <img alt="$\mathscr{E}[W_n]=1$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/fcdab563c7e2c08db88ec87357bdf89e.svg?invert_in_darkmode" align="middle" width="76.82466pt" height="24.6576pt" style="position:relative;top:10px"/> and <img alt="$\mathscr{E}[\mathcal{L}(w,\theta) ] = \mathcal{L}(\theta) $" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/55f80d456ef09ccdfbafd873084115d6.svg?invert_in_darkmode" align="middle" width="128.242785pt" height="24.6576pt" style="position:relative;top:10px"/>.
There is a proof that the coreset log-likelihood <img alt="$\mathcal{L}(w,\theta) $" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/9acd6d22f0da8977311fbbe36e7de4a7.svg?invert_in_darkmode" align="middle" width="51.81363pt" height="24.6576pt" style="position:relative;top:10px"/> satisfies Eq. 2 with high probability and <img alt="$\epsilon^2 = \mathcal{O}(\frac{1}{M})$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/1100b6e726fa96fc9584547b63daea13.svg?invert_in_darkmode" align="middle" width="80.008005pt" height="27.77577pt" style="position:relative;top:10px"/>.
The other interesting formulation for this is to sparse vector sum approximation:

<p align="center"><img alt="$$&#10;\underset{w \in \mathscr{R}^N}{\mathrm{argmin}} ||\mathcal{L}(w) - \mathcal{L} ||^2&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/59248ffbbd0cbc63a8413863084a0082.svg?invert_in_darkmode" align="middle" width="142.854855pt" height="29.913675pt" style="position:relative;top:10px"/></p>
<p align="center"><img alt="$$&#10;s.t. w\geq 0, \sum{n=1}^{N} \mathscr{1} [w_n &gt; 0] \leq M&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/42adb69ecfbec0c307269118f5b34796.svg?invert_in_darkmode" align="middle" width="259.24635pt" height="26.30166pt" style="position:relative;top:10px"/></p>
this theorem depends on quantitiy <img alt="$0 \leq \hat{\etha}\leq 2$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/54e235fec0678e136145da6f09c2eee1.svg?invert_in_darkmode" align="middle" width="60.273675pt" height="22.83138pt" style="position:relative;top:10px"/>:

<p align="center"><img alt="$$&#10;\hat{\eta} = \underset{n,m \in [N]}{\mathrm{max}} ||\frac{\mathcal{L}_n}{\sigma_n}-\frac{\mathcal{L}_m}{\sigma_m}||&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/648108588ff502a28395cd39c10ae317.svg?invert_in_darkmode" align="middle" width="172.6692pt" height="36.600135pt" style="position:relative;top:10px"/></p>
that indicates how well the vectors are aligned. If it's zero it means the error is zero since we can approximate all <img alt="$\mathcal{L}$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/47291815667dfe5994c54805102e144b.svg?invert_in_darkmode" align="middle" width="11.337975pt" height="22.46574pt" style="position:relative;top:10px"/> with single vector  <img alt="$\mathcal{L}_n$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/1818eca8fbb5a307ec0f5436121fd2b2.svg?invert_in_darkmode" align="middle" width="19.46406pt" height="22.46574pt" style="position:relative;top:10px"/>. 
So the uniform coreset construction with probability <img alt="$\geq 1-\delta$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/439d02a63594f85ba6b9263ef6057948.svg?invert_in_darkmode" align="middle" width="53.59002pt" height="22.83138pt" style="position:relative;top:10px"/> satisfies:

<p align="center"><img alt="$$&#10;||\mathcal{L}(W)-\mathcal{L}|| \leq \frac{\sigma}{M}(\text{dim}(\mathcal{L}_n)_{n=1}^N+\hat{\eta}\sqrt{2\log \frac{1}{\delta}})&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/51de9d63236bc70122f825efcff70b0a.svg?invert_in_darkmode" align="middle" width="326.45085pt" height="39.45249pt" style="position:relative;top:10px"/></p>

where <img alt="$\text{dim}(u_n)_{n=1}^N$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/68592ae4d05130e4fc75690abb2a82e7.svg?invert_in_darkmode" align="middle" width="83.310975pt" height="27.65697pt" style="position:relative;top:10px"/> of <img alt="$N$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/f9c4988898e7f532b9f826a75014ed3c.svg?invert_in_darkmode" align="middle" width="14.94405pt" height="22.38192pt" style="position:relative;top:10px"/> vectors in a normed vector space is the minimum value of <img alt="$d\in N$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/d4f3107d7bd94831a02b4d84d883972d.svg?invert_in_darkmode" align="middle" width="43.64712pt" height="22.83138pt" style="position:relative;top:10px"/> such that all vectors can be approximated using linear combinations of a set of <img alt="$d$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/2103f85b8b1477f430fc407cad462224.svg?invert_in_darkmode" align="middle" width="8.556075pt" height="22.83138pt" style="position:relative;top:10px"/> unit vectors <img alt="$(v_j)_{j=1}^N$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/aeb96d324f25e322cf5ff8421d01f4fb.svg?invert_in_darkmode" align="middle" width="50.428455pt" height="27.65697pt" style="position:relative;top:10px"/>, <img alt="$|v_j|=1$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/ac1625745434a3a019e26e2c8e6b5a34.svg?invert_in_darkmode" align="middle" width="54.16389pt" height="24.6576pt" style="position:relative;top:10px"/>.

Let's stop here and take a look at Fig. 1. As you see uniform sampling doesn't have sense of directionality. As you see important sampling (based on thickness of curves and vectors) captures this effect even though it might be far away from the posterior distribution (blue curve and blue vector).

In this word they proposed a Hilbert set of coresets that minimizes:
<p align="center"><img alt="$$&#10;E[||\mathcal{L}(W)-\mathcal{L}||^2] = \frac{\sigma^2\eta^2}{M} , \eta = 1- \frac{||\mathcal{L}||^2}{\sigma^2}&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/8463ff1a57d54b89ac2e83d83d5ee6cd.svg?invert_in_darkmode" align="middle" width="285.7965pt" height="35.777445pt" style="position:relative;top:10px"/></p>
where similarly <img alt="$0\leq \eta\leq1$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/b6831d4e89460a5073e7e64846da9748.svg?invert_in_darkmode" align="middle" width="69.025605pt" height="21.18732pt" style="position:relative;top:10px"/> captures how well the vectors <img alt="$\mathcal{L}_n$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/1818eca8fbb5a307ec0f5436121fd2b2.svg?invert_in_darkmode" align="middle" width="19.46406pt" height="22.46574pt" style="position:relative;top:10px"/> are aligned. ||.|| is the Hillbert norm.
The probability <img alt="$\geq 1-\delta$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/439d02a63594f85ba6b9263ef6057948.svg?invert_in_darkmode" align="middle" width="53.59002pt" height="22.83138pt" style="position:relative;top:10px"/>:

<p align="center"><img alt="$$&#10;||\mathcal{L}(W)-\mathcal{L}|| \leq \frac{\sigma}{M}\Big( \eta + \eta_M\sqrt{2\log \frac{1}{\delta}} \Big)&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/cd90481f69ce1f1dd55dbb606ac69bc9.svg?invert_in_darkmode" align="middle" width="269.99445pt" height="39.45249pt" style="position:relative;top:10px"/></p>
where <img alt="$\eta_M$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/30c082a2a14c1eb21b57d77cd0caf723.svg?invert_in_darkmode" align="middle" width="21.931635pt" height="14.15535pt" style="position:relative;top:10px"/> is a constant defined in the paper. 

The most fancy algorithm proposed by the author is to select the samples more intelligently towards the direction of greatest improvement. So the Frank-Wolf coreset construction is defined first by a cardinality constraint on <img alt="$w$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/31fae8b8b78ebe01cbfbe2fe53832624.svg?invert_in_darkmode" align="middle" width="12.21099pt" height="14.15535pt" style="position:relative;top:10px"/>:

<p align="center"><img alt="$$&#10;\underset{w \in \mathscr{R}^N}{\mathrm{min}} (w-1)^T K(w-1)&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/44af8eee94cf067021314782aa9c889a.svg?invert_in_darkmode" align="middle" width="171.30465pt" height="27.16494pt" style="position:relative;top:10px"/></p>
<p align="center"><img alt="$$&#10;s.t., w\geq 0, \sum_{n=1}^{N} \sigma_n w_n =\sigma&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/f7f9245b91615ab70248477521fcea9e.svg?invert_in_darkmode" align="middle" width="178.20165pt" height="47.60745pt" style="position:relative;top:10px"/></p>
where <img alt="$K_{i,j} = &lt;\mathcal{L}_i,\mathcal{L}_j&gt;$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/31ea30a047ee5a8eb6d0257568b8bb60.svg?invert_in_darkmode" align="middle" width="123.878205pt" height="22.46574pt" style="position:relative;top:10px"/>. It's optimal with <img alt="$w=[1,..,1]^T\in \mathscr{R}^N$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/4b5b92c0265e5d39daa40ef74a7c8ace.svg?invert_in_darkmode" align="middle" width="138.14493pt" height="27.65697pt" style="position:relative;top:10px"/> and have vertices <img alt="$\frac{\sigma}{\sigma_n}1_n$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/da3d97677864de1c68e094072a3d2e69.svg?invert_in_darkmode" align="middle" width="33.996765pt" height="22.85349pt" style="position:relative;top:10px"/> where <img alt="$1_n$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/d711aabe1ffe4eb1668aa065cf89a496.svg?invert_in_darkmode" align="middle" width="16.34523pt" height="21.18732pt" style="position:relative;top:10px"/> is the <img alt="$n$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/55a049b8f161ae7cfeb0197d75aff967.svg?invert_in_darkmode" align="middle" width="9.83004pt" height="14.10255pt" style="position:relative;top:10px"/>-th coordinate unit vector. 
So this algorithm adds at most one data point to the coreset at each iteration. So:

<p align="center"><img alt="$$&#10;w_0 = \frac{\sigma}{\sigma_{f_0}} 1_{f_0}, f_0 = \underset{n\in [N]}{\mathrm{arg max}}  &lt;\mathcal{L},\frac{1}{\sigma_n}\mathcal{L}_n&gt;&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/2f6ddd86537173943332a25e000090b8.svg?invert_in_darkmode" align="middle" width="288.51405pt" height="39.15714pt" style="position:relative;top:10px"/></p>

The interesting part is that since gradient of cost is <img alt="$2K(w_t-1)$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/c89060f2f2b1253a87e93a494c6fa952.svg?invert_in_darkmode" align="middle" width="82.0083pt" height="24.6576pt" style="position:relative;top:10px"/> in iteration <img alt="$t$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/4f4f4e395762a3af4575de74c019ebb5.svg?invert_in_darkmode" align="middle" width="5.9361555pt" height="20.22207pt" style="position:relative;top:10px"/>, the algorithm's direction is:

<p align="center"><img alt="$$&#10;d_t = \frac{\sigma}{\sigma_{f_0}}1_{f_0}-w_t,  f_t = \underset{n\in [N]}{\mathrm{arg max}}  &lt;\mathcal{L} - \mathcal{L}(w_t),\frac{1}{\sigma_n}\mathcal{L}_n&gt;&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/e8e653a9a15f35d3e3cd4590b8fa2a58.svg?invert_in_darkmode" align="middle" width="381.546pt" height="39.15714pt" style="position:relative;top:10px"/></p>

they update <img alt="$w_t$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/dde30cc90adc3d7de889d34c65ca6f25.svg?invert_in_darkmode" align="middle" width="16.7343pt" height="14.15535pt" style="position:relative;top:10px"/> by this:
<p align="center"><img alt="$$&#10;w_{t+1} = w_t + \gamma_t d_t&#10;$$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/7d1a3592ad9f1ff98d777261455284b3.svg?invert_in_darkmode" align="middle" width="121.5852pt" height="15.2511315pt" style="position:relative;top:10px"/></p>
where <img alt="$\gamma_t$" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/69304177ce432541b67b103783dfade3.svg?invert_in_darkmode" align="middle" width="13.47654pt" height="14.15535pt" style="position:relative;top:10px"/> is a constant defined on the paper. 
```
@article{Campbell:2017,
  title={Automated Scalable Bayesian Inference via Hilbert Coresets},
  author={Campbell, Trevor and Broderick, Tamara},
  journal={arXiv preprint arXiv:1710.05053},
  year={2017}
}
```
