---
layout: post
title: Zero-shot Transfer Learning for Semantic Parsing
---

This paper is one of my memorabale works in LILY group particularly with Alexander Fabbri.
In 2017 in John Lafferty's non-parametric estimation course we were introduced to the award winning paper by Percy Liang's group. 
There is quit a lot of material in deep learning but you can't find really that much works about openning this black box. 
Inspired by 'Understanding Black-box Predictions via Influence Functions' we started expanding our running works on semantic parsing. 

In this blog post we don't need to invoke any complicated ideas in semantic parsing but i'mg going to give enough background for you to follow the rest. 
In classical definition semantic parsing is defined as generating logical forms of a sequence of a human language. 
You can think of this as outputing sequences of lambda expressions, SQL queries, or programming languages. 
Our prediction is obtained through a large number of text-text pairs interpreted as squence-sequence pairs in a neural network or markov chain setting.
In this paper we try to figure out how to use information in different domains in order to boos the performance of the running domain. 
Let's say you have many examples in publication domain but a few in education. For example 1000 pairs in first domain and 100 in the second one.
If I assume publications is partly related to education, use of it as training data seems almost necessary. 
But how much and which examples, this is the motivation of this paper. 

<table style="width:100%">
  <tr>
    <th>Firstname</th>
    <th>Lastname</th> 
    <th>Age</th>
  </tr>
  <tr>
    <td>Jill</td>
    <td>Smith</td> 
    <td>50</td>
  </tr>
  <tr>
    <td>Eve</td>
    <td>Jackson</td> 
    <td>94</td>
  </tr>
</table>
