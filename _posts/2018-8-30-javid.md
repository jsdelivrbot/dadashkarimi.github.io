---
layout: post
title: Zero-shot Transfer Learning for Semantic Parsing
---

This paper is one of my memorabale works in LILY group particularly with Alexander Fabbri.
In 2017 in John Lafferty's non-parametric estimation course we were introduced to the award winning paper by Percy Liang's group. 
There is quit a lot of material in deep learning but there is not really that much works openning this black box. 
Inspired by 'Understanding Black-box Predictions via Influence Functions' we started expanding our running works on semantic parsing. 

In this blog post we don't need to invoke any complicated ideas but i'mg going to give enough background for you to follow the rest. 
In classical definition semantic parsing is defined by a task generating logical forms of a sequence of a human language. 
You can think of this as outputing sequences of lambda expressions, SQL queries, or programming languages. 
Our prediction is obtained through a large number of text-text pairs interpreted as squence-sequence pairs in a neural network or markov chainsetting.
In this paper we try to figure out how to use information in different domains in order to boos the performance of the running domain. 
Let's say you have many examples in publication domain but a few in education.
If I assume publications is partly related to education, use of it as training data seems almost necessary. 
But how much and which examples, this is the motivation of this paper. 

<p align="center"><img alt="\begin{equation}&#10;&#10;\begin{figure*}[t]&#10;    \centering&#10;    \begin{subfigure}[t]{0.48\textwidth}&#10;        \centering&#10;        \includegraphics[width=\linewidth]{fig/sing-agg-pub.pdf}&#10;        \caption{publications}&#10;    \end{subfigure}%&#10;    ~ &#10;    \begin{subfigure}[t]{0.48\textwidth}&#10;        \centering&#10;        \includegraphics[width=\linewidth]{fig/sing-agg-cal.pdf}&#10;        \caption{calendar}&#10;    \end{subfigure}&#10;    ~&#10;    \begin{subfigure}[t]{0.48\textwidth}&#10;        \centering&#10;        \includegraphics[width=\linewidth]{fig/m2m-o2m-e2d-pub.pdf}&#10;        \caption{publications}&#10;    \end{subfigure}%&#10;    ~ &#10;    \begin{subfigure}[t]{0.48\textwidth}&#10;        \centering&#10;        \includegraphics[width=\linewidth]{fig/m2m-o2m-e2d-cal.pdf}&#10;        \caption{calendar}&#10;    \end{subfigure}&#10;    \caption{Denotation level accuracy for single-task training (i.e., train on task A, test on task A) and multi-task aggregation training (i.e., train on all tasks, test on task A) as a function of training size on the same random shuffle of the data are shown in (a) and (b). A comparison of accuracy for the many to many, one to many, the encoder decoder methods in \cite{Herzig:17} and our proposed zero-shot k*d method as a function of training size are shown in (c) and (d).}&#10;    \label{fig:sing-agg}&#10;\end{figure*}&#10;\end{equation}" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/7a6adb3273ff3d954638f07d1fe7573d.svg?sanitize=true" align="middle" width="699.35085pt" height="438.02055pt"/></p>



<p align="center"><img alt="\begin{table}[h]&#10;\centering&#10;\scalebox{0.72}{&#10;\begin{tabular}{lll}&#10;\cline{1-3}&#10;method &amp; \#parameters &amp; reference\\\cline{1-3}&#10;o2o &amp;$\mathcal{O}(d^2+dV)$&amp; \citet{Johnson:2016}\\&#10;o2m &amp;$\mathcal{O}(d^2+kdV)$&amp; \citet{Fan:17}\\&#10;m2m&amp;$\mathcal{O}((k+1)d^2+kdV)$&amp; \citet{Daume:2009}\\&#10;e2d&amp;$\mathcal{O}(d^2+dV)$&amp;\citet{Herzig:17} \\&#10;z-shot k*d &amp;$\mathcal{O}(d^2+dV)$&amp; Our Method\\&#10;\cline{1-3}&#10;\end{tabular}}&#10;\caption{A comparison of the number of training parameters for multiple transfer learning models for zero-shot learning, where $d$ represents the size of the hidden dimension, $k$ the number of tasks and $V$ the size of the vocabulary.}&#10;\label{tab:parameters}&#10;\end{table}" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/ef7c79e9579cac4f3583fbe09a0b6511.svg?sanitize=true" align="middle" width="697.9764pt" height="190.49085pt"/></p>


```
@ARTICLE{Javid:20018,
   author = {{Dadashkarimi}, J. and {Fabbri}, A. and {Tatikonda}, S. and 
  {Radev}, D.~R.},
    title = "{Zero-shot Transfer Learning for Semantic Parsing}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1808.09889},
 primaryClass = "cs.CL",
 keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
     year = 2018,
    month = aug,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180809889D},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
```
