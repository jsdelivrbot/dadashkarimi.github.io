---
layout: post
title: Zero-shot Transfer Learning for Semantic Parsing
---

This paper is one of my memorabale works in LILY group particularly with Alexander Fabbri.
In 2017 in John Lafferty's non-parametric estimation course we were introduced to the award winning paper by Percy Liang's group. 
There is quit a lot of material in deep learning but there is not really that much works openning this black box. 
Inspired by 'Understanding Black-box Predictions via Influence Functions' we started expanding our running works on semantic parsing. 

In this blog post we don't need to invoke any complicated ideas but i'mg going to give enough background for you to follow the rest. 
In classical definition semantic parsing is defined by a task generating logical forms of a sequence of a human language. 
You can think of this as outputing sequences of lambda expressions, SQL queries, or programming languages. 
Our prediction is obtained through a large number of text-text pairs interpreted as squence-sequence pairs in a neural network or markov chainsetting.
In this paper we try to figure out how to use information in different domains in order to boos the performance of the running domain. 
Let's say you have many examples in publication domain but a few in education.
If I assume publications is partly related to education, use of it as training data seems almost necessary. 
But how much and which examples, this is the motivation of this paper. 

Let <img alt="$T=\{t_1,..,t_K\}$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/994984653d3a65c62ed9495be497f9d0.svg?sanitize=true"/> be the set of <img alt="$K$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/d6328eaebbcd5c358f426dbea4bdbf70.svg?sanitize=true"/> tasks and <img alt="$S_k=\{(\newvec{x}_l,\newvec{y}_l,t_k)\}_{l=1}^{N_{k}}$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/78931c004865a6eabb0551dd5d4a00dc.svg?sanitize=true"/> the set of <img alt="$N_{k}$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/75f32dec48b8afd16548b1be26105644.svg?sanitize=true"/> examples in <img alt="$t_k \in T$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/3b2c20c19a4b42c541c1ab8ae31a1caa.svg?sanitize=true"/>. We can define the probability of generating token <img alt="$w$" style="position:relative; top:2px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/31fae8b8b78ebe01cbfbe2fe53832624.svg?sanitize=true"/> by:
<p align="center"><img alt="\begin{align*}&#10;    p(y_j=w|x,y_{1:j-1}) &amp;= \\ \sum\limits_{k=1}^{K}p(y_j=w|x,y_{1:j-1},t_k)&amp;p(t_k|x,y_{1:j-1}),&#10;\end{align*}" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/29b2ad22536a27ebd593bd2b03a25cfc.svg?sanitize=true" align="middle" width="284.2851pt" height="74.602605pt"/></p>
where the first part of the summation can be estimated by \citet{Jia:2016}:
<p align="center"><img alt="\begin{eqnarray*}&#10;    p(y_j=w|x,y_{1:j-1},t_k) &amp;\propto \exp (U_w[s_j,c_j]) \\ &#10;    p(y_j=\textrm{copy}[i]|x,y_{1:j-1},t_k) &amp;\propto \exp (e_{j,i}) &#10;\end{eqnarray*}" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/be14e3f36c172467edb37a608c72e570.svg?sanitize=true" align="middle" width="328.7361pt" height="41.63511pt"/></p>
where <img alt="$U_w$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/16831da1c588eed8782f81c5be0ffc94.svg?sanitize=true"/> is a general decoder matrix over <img alt="$T$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/2f118ee06d05f3c2d98361d9c30e38ce.svg?sanitize=true"/>; <img alt="$s_j\in \mathbb{R}^{k\times 2d}$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/510d58ae0b056d9daa5c6219eef657da.svg?sanitize=true"/> and <img alt="$c_j\in \mathbb{R}^{k\times 2d}$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/e69d7a7360b93c5220b105f6a9837b81.svg?sanitize=true"/> are the decoder and context states respectively.
The case where we copy token <img alt="$i$" style="position:relative; top:2px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/77a3b857d53fb44e33b53e4c8b68351a.svg?sanitize=true"/> of the tokens from the source sequence is copy<img alt="$[i]$" style="position:relative; top:2px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/3dc0c8cb462b1c435f7efa7285ae1dc6.svg?sanitize=true"/> and
<img alt="$e_{j,i}$" style="position:relative; top:2px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/e288063dfc79d6fccad80ad116147133.svg?sanitize=true"/> is the attention score of the current state <img alt="$j$" style="position:relative; top:2px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/36b5afebdba34564d884d347484ac0c7.svg?sanitize=true"/> with respect to the source position <img alt="$i$" style="position:relative; top:2px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/77a3b857d53fb44e33b53e4c8b68351a.svg?sanitize=true"/>.

The second part is:
<p align="center"><img alt="\begin{eqnarray*}&#10;    p(t_k|x,y_{1:j-1}) \propto \exp (W_{t_k}[s_j,c_j])&#10;    \label{eq:word-task}&#10;\end{eqnarray*}" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/197d4c3deab5c6041f34f2104f134d76.svg?sanitize=true" align="middle" width="231.3993pt" height="16.97751pt"/></p>
where <img alt="$W_T \in \mathbb{R}^{K\times 4d}$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/bf754dbe24ae16cf0f0c13f6adda85e3.svg?sanitize=true"/> is a mapping between the current state and all the domains. 
Eq. \ref{eq:word-task} gives scores over the <img alt="$k$" style="position:relative; top:2px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/63bb9849783d01d91403bc9a5fea12a2.svg?sanitize=true"/> domains.

% scores for all <img alt="$t_k\in T$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/6e67f75bab8190574f6ee464bba0bfda.svg?sanitize=true"/> where only one is the gold task.
%and the others are related tasks based on their scores. 
We predict the task for the current decoder state <img alt="$j$" style="position:relative; top:2px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/36b5afebdba34564d884d347484ac0c7.svg?sanitize=true"/> as follows:
<p align="center"><img alt="\begin{eqnarray*}&#10;  \hat{k}= \arg\max_k ~W_{t_k}[s_j,c_j].&#10;\end{eqnarray*}" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/be7a2a8eb91e46612ba61b01ac71ea98.svg?sanitize=true" align="middle" width="173.5965pt" height="26.441415pt"/></p>
Then we assign <img alt="$\hat{k}$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/49aba54ceee6ccc063d260ddeee91092.svg?sanitize=true"/> for example <img alt="$x$" style="position:relative; top:2px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?sanitize=true"/> and use the corresponding <img alt="$p(y_j=w|x,y_{1:j-1},t_{\hat{k}})$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/bc00b349a3c163335b85043caa042902.svg?sanitize=true"/> for prediction. 
% In our experiments with sparse training data we set <img alt="$p(t_{\hat{k}}|x,y_{1:j-1})=1$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/1ffae3dae263ff5c5cf731745447e809.svg?sanitize=true"/> and computed <img alt="$p(y_j=w|x,y_{1:j-1},t_{\hat{k}})$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/bc00b349a3c163335b85043caa042902.svg?sanitize=true"/> by <img alt="$U_w=W_{t_k}[t_{\hat{k}}]W_{\textrm{out}}$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/6199fee95e2285c562bf84f5e1061b2e.svg?sanitize=true"/> where <img alt="$W_{t_k}[t_{\hat{k}}]\in \mathbb{R}^{1\times 4d}$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/1498a1ad8a4abd41ba449d9ddc369107.svg?sanitize=true"/> is a slice of the mapping matrix and <img alt="$W_{\textrm{out}} \in \mathbb{R}^{4d\times |V|}$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/40b2c0b50e1fe896ee936e4def3b184f.svg?sanitize=true"/>.

We also define <img alt="$\hat{t}_{k'}$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/5c8a7f0f0ec9358ee985bf43c1d50dd1.svg?sanitize=true"/> as the task predicted for our encoder (i.e., <img alt="$\arg\max_{k'} W_{t_{k'}}[b_n,c_n]$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/7dc157a11adcc09745a73fe8ad2efc8f.svg?sanitize=true"/> where <img alt="$n=|x|$" style="position:relative; top:7px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/30af83ccaeeba4bb6a42c23ce2549116.svg?sanitize=true"/> or the size of input).
Finally the loss function is:
<p align="center"><img alt="\begin{eqnarray*}&#10;  \label{eq:loss}&#10;  \mathcal{L} = &amp;-\log p(y_{1:j}) + \frac{1}{2}||t_k-\hat{t}_k||^2+\frac{1}{2}||t_k-\hat{t}_{k'}||^2&#10;\end{eqnarray*}" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/9b9bed26ca990ffed9ae8d0ae9767d0b.svg?sanitize=true" align="middle" width="342.7479pt" height="20.074725pt"/></p>
where the first term is standard negative log likelihood and the second and the third terms penalize misclassifications of the gold task <img alt="$t_k$" style="position:relative; top:2px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/509bf7d4f0f63616580a39c4ed8b527d.svg?sanitize=true"/>.

By following the above formulation our model tries to estimate a distribution over <img alt="$k$" style="position:relative; top:2px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/63bb9849783d01d91403bc9a5fea12a2.svg?sanitize=true"/> tasks/topics for each word (tasks are indicated by <img alt="$t_{1:k}$" style="position:relative; top:2px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/aa8e457a1ba1a4a5c9303a9d05b44225.svg?sanitize=true"/> for the input example <img alt="$x_t$" style="position:relative; top:2px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/23776aad854f2d33e83e4f4cad44e1b9.svg?sanitize=true"/> with initial state <img alt="$h_{t-1}$" style="position:relative; top:2px;" src="https://rawgit.com/dadashkarimi/dadashkarimi.github.io/master/svgs/0536b5a8288f3bf4107cbb3af2c23191.svg?sanitize=true"/>). 

```
@ARTICLE{Javid:20018,
   author = {{Dadashkarimi}, J. and {Fabbri}, A. and {Tatikonda}, S. and 
  {Radev}, D.~R.},
    title = "{Zero-shot Transfer Learning for Semantic Parsing}",
  journal = {ArXiv e-prints},
archivePrefix = "arXiv",
   eprint = {1808.09889},
 primaryClass = "cs.CL",
 keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
     year = 2018,
    month = aug,
   adsurl = {http://adsabs.harvard.edu/abs/2018arXiv180809889D},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
```
